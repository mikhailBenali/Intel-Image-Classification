{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification of the \"Intel Image Classification\" Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Start by importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "from random import randint as rand\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./Data/seg_train/seg_train\")\n",
    "\n",
    "img = []\n",
    "targets = []\n",
    "\n",
    "for folder in path.iterdir():\n",
    "    for file in folder.iterdir():\n",
    "        img.append(v2.Resize((150,150))(read_image(str(file))))\n",
    "        targets.append(folder.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.stack(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = np.unique(targets)\n",
    "targets = torch.tensor([np.where(target_dict == target)[0][0] for target in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img, targets):\n",
    "        self.img = img.clone().detach().to(device).to(torch.float32)\n",
    "        self.targets = targets.clone().detach().to(device).to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img[idx]\n",
    "        target = self.targets[idx]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageDataset(img, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rand(0, len(img))\n",
    "plt.imshow(v2.ToPILImage()(img[r]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[r]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data targets:\n",
    "\n",
    "0. buildings\n",
    "1. forest\n",
    "2. glacier\n",
    "3. mountain\n",
    "4. sea\n",
    "5. street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # Input size - Kernel size / Stride + 1 = Output size (6,146,146)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # Divide the size by 2 (6,73,73)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # (16,69,69) -> (16,34,34) (Because of the pooling layer (again)) -> Flatten (16*34*34) = 18496\n",
    "        \n",
    "        #Dense layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 34 * 34, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.targets.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size=4, shuffle=True)\n",
    "\n",
    "epoch = 100\n",
    "time_start = perf_counter()\n",
    "for e in range(epoch):\n",
    "    for i, (img, target) in enumerate(loader):\n",
    "        target = target.long()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "time_end = perf_counter()\n",
    "print(f\"Time taken: {time_end - time_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    return target_dict[torch.argmax(model(img)).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data targets:\n",
    "\n",
    "0. buildings\n",
    "1. forest\n",
    "2. glacier\n",
    "3. mountain\n",
    "4. sea\n",
    "5. street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    r = rand(0, len(data.img))\n",
    "    print(f\"{r = }\")\n",
    "    print(f\"Predicted: {predict(data.img[r].unsqueeze(0))}\")\n",
    "    print(f\"Real: {target_dict[int(data.targets[r])]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for i in range(10000):\n",
    "    r = rand(0, len(data.img))\n",
    "    if predict(data.img[r].unsqueeze(0)) == target_dict[int(data.targets[r])]:\n",
    "        accuracy += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy/10000*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs / Accuracy: \n",
    "\n",
    "- 10 : 18%\n",
    "- 30 : 30%\n",
    "- 50 : 60%\n",
    "- 100 : 70%\n",
    "- 150 : 18%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
